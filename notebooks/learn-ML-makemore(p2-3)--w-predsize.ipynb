{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccaef4fa-ef46-4715-8046-2c1847a24837",
   "metadata": {},
   "source": [
    "# Course exercise \n",
    "\n",
    "During part 2-3 of `Building makemore` course [Neural Networks: Zero to Hero](https://github.com/karpathy/nn-zero-to-hero/tree/master) from Andrej Karpathy we've built an [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron).\n",
    "\n",
    "At the end of Part 2, he suggested that we explore ways to improve the results he obtained, which included a `loss` of `2.17`. This could be achieved by adjusting the number of dimensions for the character embedding vectors, the number of neurons in the hidden layer, and other parameters.\n",
    "\n",
    "I achieved some impressive scores, and here is an example:\n",
    "\n",
    "```\n",
    "Train loss is : 1.805803894996643\n",
    "Valid loss : 1.8799562454223633\n",
    "Test loss: 1.8769153356552124\n",
    "\n",
    "block_size: 2\n",
    "pred_block_size: 2\n",
    "Embeding layers : 6\n",
    "Hidden layer: 100\n",
    "NB_Epoch : 200000\n",
    "No profound strategy for lr  (same lr decay): lr = 0.1 if i < 100000 else 0.01\n",
    "Generator : manual_seed(2147483647)\n",
    "Minibatch: 256\n",
    "```\n",
    "\n",
    "In this notebook, you'll find my contribution to the original code. I introduced pred_block_size, which is used to make predictions not only on a single character but on a quantity that we can adjust.\n",
    "\n",
    "As a beginner in ML, I can't definitively say whether this is a brilliant idea, but I've observed that the model can quickly overfit if the dataset used contains a lot of short words. The increased `vocab` size might also impact performance.\n",
    "\n",
    "Additionally, I incorporated two other datasets consisting of French and Indian names to compare the solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92114857-6b45-4227-b59f-eef53de7997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42f634-cf33-4554-b1e0-d66dfd1635de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN-US raw data\n",
    "#rawwords = open('names.csv', 'r').read()\n",
    "#words_ha = rawwords.split(',')\n",
    "#words = [ w.strip('\\\"') for w in words_ha]\n",
    "#len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe24c7-f6fd-4d45-8483-d486d2b2ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FR raw data\n",
    "#df = pd.read_csv('liste_des_prenoms.csv', sep=';') # file has ; as separator instead of ,\n",
    "#words = df['Prenoms'].tolist()\n",
    "#len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1349cf9b-5a33-411c-8359-4edc49dd662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6486"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INDIAN raw data\n",
    "df = pd.read_csv('Indian_Names.csv', )\n",
    "df['Name'] = df['Name'].astype(str) # clean data\n",
    "words = df['Name'].astype(str).tolist()\n",
    "len(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e4efe55-fb34-490c-a8c4-74feffa723a3",
   "metadata": {},
   "source": [
    "### Dataset settings\n",
    "The `block_size` variable is used to create the `X` values and represents the number of consecutive characters our predictions will be based on.\n",
    "\n",
    "`pred_block_size` is the additional variable that I have incorporated into the original exercise. This one influences the values of our `Y` tensors. As a result, the inference can now be made on a customizable number of characters.\n",
    "\n",
    "While testing this idea, I discovered that the `loss` result improved, but the generative results exhibited some overfitting, with a significant number of values not being original new words. You'll see how this issue is addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eb217695-1643-4017-94eb-c6f95e722323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.356614246068455"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choosing settings\n",
    "# With regard of choosing the right block_size & pred_block_size. We can inspect the average lenght of words of our dataset.\n",
    "alw =  sum([ len(w) for w in words]) / len(words) # average lenght word \n",
    "alw\n",
    "# EN -> 6.12\n",
    "# FR -> 5.72\n",
    "# IND -> 6.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d950473d-6c85-4d7c-8c35-5b9c3db7749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset settings\n",
    "block_size = 3 # context char length \n",
    "pred_block_size = 3 # prediction char lenght\n",
    "padd_size = (pred_block_size - 1) # padding size\n",
    "pred_context = '.' * padd_size "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73ac0a46-e769-4c91-bed0-5330ecb96db7",
   "metadata": {},
   "source": [
    "### Build the vocabulary of characters and mappings to/from integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f23d5d10-7424-4be7-a56f-8c38d92ae64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3884"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbs_id = ['.'] * pred_block_size \n",
    "pbs_id_t = tuple(pbs_id)\n",
    "\n",
    "# extra function since we have var pred_block_size\n",
    "def build_vocab(words,pred_block_size): \n",
    "    vocab_context = []\n",
    "    \n",
    "    for w in words:\n",
    "        context = ['.'] * pred_block_size \n",
    "        formated_word = '.' + w + pred_context # apply contextual padding\n",
    "        \n",
    "        for pos,ch in enumerate(formated_word):\n",
    "            context = context[1:] + [ch] # crop and append\n",
    "            vocab_context.append(tuple(context))\n",
    "            \n",
    "    return vocab_context\n",
    "\n",
    "vocab_build = build_vocab(words, pred_block_size) \n",
    "predChars = sorted(list(set(vocab_build)))\n",
    "#monochars = sorted(list(set(''.join(words))))\n",
    "\n",
    "#predChars\n",
    "stoi = {s:i+1 for i,s in enumerate(predChars)} \n",
    "stoi.update({pbs_id_t:0}) # adding 0 index value to dict\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "len(predChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0b5b5a71-90f9-4e45-8617-ece3ddb3e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(itos)\n",
    "#stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ad4b975b-e0e2-45a2-8be1-4412b608c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add + 1 for . (index 0) val char\n",
    "vocab_size = len(predChars) + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9064bc34-bda7-408d-8ba3-7a6a018563e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createContextY(idx,fWord, predBlockSize):\n",
    "    contextY = []\n",
    "    for j in range(predBlockSize):\n",
    "        \n",
    "        if idx+j <= (len(fWord)-1):\n",
    "            contextY.append(fWord[idx+j])\n",
    "        else:\n",
    "            contextY = ['.'] * predBlockSize\n",
    "    return contextY\n",
    "\n",
    "def contextToI(context):\n",
    "    icontext = []\n",
    "    return stoi[tuple(context)]\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words: \n",
    "        context = [0] * block_size\n",
    "        formated_word = pred_context + w + '.'  # apply padding\n",
    "        \n",
    "        for pos, ch in enumerate(formated_word):\n",
    "            if pos < len(formated_word) - pred_block_size + 1:\n",
    "                v = tuple(formated_word[pos:pos+pred_block_size])\n",
    "                ix = stoi.get(v, \"Key not found in dictionary\")\n",
    "            else:\n",
    "                l = [ch]\n",
    "                l.extend(['.'] * padd_size)\n",
    "                v = tuple(l)\n",
    "                ix = stoi[v]\n",
    "            \n",
    "            context_pred = createContextY(pos,formated_word,pred_block_size)     \n",
    "            iy = contextToI(context_pred) \n",
    "            X.append(context)\n",
    "            Y.append(iy)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "                \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6e9246c4-b21a-45e8-b13e-a452c7e8be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating seed, suffle and divide data set in parts training 80%, validation 10%, test 10%\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b92be015-1f01-4746-8fd8-5fae94c6be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48500, 3]) torch.Size([48500])\n",
      "torch.Size([6115, 3]) torch.Size([6115])\n",
      "torch.Size([6072, 3]) torch.Size([6072])\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets  \n",
    "Xtr, Ytr = build_dataset(words[:n1]) # training set, 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # validation set, 10%\n",
    "Xte, Yte = build_dataset(words[n2:]) # test set, 10%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b167b03-0deb-4954-8293-cd882de019a5",
   "metadata": {},
   "source": [
    "### I did not use batch normalization\n",
    "I chose to not apply batch normalization, since I don't understand it very well yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "836be687-cc3e-40f6-a12e-7e5a67d234f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 417595\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "nb_embed = 6 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "C = torch.randn((vocab_size, nb_embed), generator=g) \n",
    "W1 = torch.randn(((nb_embed*block_size), n_hidden), generator=g) * (5/3)/((nb_embed * block_size)**0.5) # * is for init optimization and (5/3) bythebooks value,see details about init opt.\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(f\"Number of parameters: {sum(p.nelement() for p in parameters)}\") # number of parameters in total\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f15c48d-4af2-440c-aa43-c142f2aa72b4",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1d227c08-7dd1-4815-8e43-25cc81ca81ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progression_step=10 %  2.7317\n",
      "progression_step=20 %  2.2137\n",
      "progression_step=30 %  1.7082\n",
      "progression_step=40 %  1.4694\n",
      "progression_step=50 %  1.5310\n",
      "progression_step=60 %  1.3851\n",
      "progression_step=70 %  1.2280\n",
      "progression_step=80 %  1.3655\n",
      "progression_step=90 %  1.3464\n",
      "1.1867951154708862\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200000\n",
    "progression_step = 10\n",
    "batch_size = 256\n",
    "\n",
    "for i in range(nb_epoch):\n",
    "        \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y of (Xtr training set)\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    h = torch.tanh(emb.view(-1, (nb_embed*block_size)) @ W1 + b1) # \n",
    "    logits = h @ W2 + b2 # \n",
    "    loss = F.cross_entropy(logits, Yb) # Here Yb is Ytr[ix] and `ix` refer to the dynamic tupple created with respect of pred_block_size\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    #lr = lrs[i] ## <<<< Use this to find the correct sweet spot value for learing rate\n",
    "    \n",
    "    # Apply a learning rate decay strategy when it plateau\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats <<<< Utility vars for learing rate finder\n",
    "    #lri.append(lr)\n",
    "    #lri.append(lre[i])\n",
    "    #lossi.append(loss.item())\n",
    "    #stepi.append(i)\n",
    "    #lossi.append(loss.log10().item())\n",
    "    \n",
    "    # printing progression in %\n",
    "    if (i*100)/nb_epoch == progression_step:\n",
    "        print(f\"{progression_step=} %  {loss.item():.4f}\")\n",
    "        progression_step += 10\n",
    "\n",
    "print(loss.item()) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed0ab04b-4139-4ac5-9796-42d3cf2ab4b9",
   "metadata": {},
   "source": [
    "### Inspect loss values for the 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "94f6d012-e473-45dd-8598-efb4cf16156c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: loss.item()= 1.2724426984786987\n"
     ]
    }
   ],
   "source": [
    "# Training set \n",
    "\n",
    "emb = C[Xtr]\n",
    "h = torch.tanh(emb.view(-1, (nb_embed*block_size)) @ W1 + b1) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "print(f'Training loss: {loss.item()= }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3e8146a9-4b00-49dc-831b-1a0cfcd2b374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()= 2.9081592559814453\n"
     ]
    }
   ],
   "source": [
    "# Validation set\n",
    "\n",
    "emb = C[Xdev] # Andrej use the term `dev` to refer to validation set\n",
    "h = torch.tanh(emb.view(-1, (nb_embed*block_size)) @ W1 + b1) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "print(f'{loss.item()= }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c5ce7c91-7bb5-45fa-b816-92bc828b5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()= 2.8859503269195557\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "\n",
    "emb = C[Xte] # verifiy with test set\n",
    "h = torch.tanh(emb.view(-1, (nb_embed*block_size)) @ W1 + b1) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, Yte)\n",
    "print(f'{loss.item()= }')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a7fc63e-aab3-4b49-ac41-238195f5384c",
   "metadata": {},
   "source": [
    "## Generate new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "655cc7f3-a2a3-4e5f-a9ea-b04a6e08e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [0] * block_size\n",
    "#C[torch.tensor([context])].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c6a57e37-d21c-4763-9420-d2b63183608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "final_render = []\n",
    "\n",
    "gen_quantity = 120\n",
    "\n",
    "for _ in range(gen_quantity):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    \n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        #print(ix)\n",
    "        context = context[1:] + [ix]\n",
    "        out.append((ix))\n",
    "        if ix == 0:\n",
    "            break\n",
    "    #print(out)\n",
    "    final_render.append(''.join(itos[i][0]  for i in out))\n",
    "\n",
    "final_render = sorted(final_render)\n",
    "clean_output = []\n",
    "\n",
    "for n in final_render:\n",
    "    new_word = n.strip('.')\n",
    "    clean_output.append(new_word)\n",
    "    #print(new_word)\n",
    "\n",
    "#print(clean_output[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8818c5d-a3a5-4156-b1e4-d6d84e5fd72a",
   "metadata": {},
   "source": [
    "### Verify overfit / originality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e43d1726-f913-4464-8034-1d22331e2a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity of NOT new words: 9\n",
      "percentage= 7.5 %\n"
     ]
    }
   ],
   "source": [
    "# I want to know amount of existing generated word\n",
    "list_existing = list(set([x for i,x in enumerate(clean_output) if x in words]))\n",
    "amount = len(list_existing) \n",
    "print(f\"Quantity of NOT new words: {amount}\")\n",
    "percentage = (amount/gen_quantity) *100\n",
    "print(f\"{percentage= } %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19dc324e-42da-4c6b-82a8-cf098ef92116",
   "metadata": {},
   "source": [
    "### Show Result\n",
    "Only the completly new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "345d7f93-e25f-4fb6-8319-f9f470be70e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity of new words generated: 110 \n",
      "\n",
      "aasi\n",
      "ai\n",
      "ajmkash\n",
      "alsma\n",
      "amarda\n",
      "amrch\n",
      "amuddi\n",
      "ane\n",
      "armjshijan\n",
      "babnoenibhu\n",
      "ban\n",
      "bhuwmohpsaraha\n",
      "biav\n",
      "bin\n",
      "binwapovla\n",
      "budpshb\n",
      "ch\n",
      "cmparruddi\n",
      "daho\n",
      "dant\n",
      "dhans\n",
      "dhanvi\n",
      "dhralta\n",
      "dolroo\n",
      "fark\n",
      "fau\n",
      "fi\n",
      "firana\n",
      "ganabl\n",
      "gyipen\n",
      "i\n",
      "iewbitbanchanmee\n",
      "inaa\n",
      "inayan\n",
      "jaa\n",
      "jayan\n",
      "jdilslahee\n",
      "jeu\n",
      "jtus\n",
      "kaenun\n",
      "kaktik\n",
      "kal\n",
      "kalpa\n",
      "kamdl\n",
      "kamruddi\n",
      "kann\n",
      "kih\n",
      "kishog\n",
      "kraiuna\n",
      "ktim\n",
      "laksarhmdev\n",
      "mandh\n",
      "mea\n",
      "min\n",
      "misn\n",
      "mnripmra\n",
      "munku\n",
      "munn\n",
      "nam\n",
      "narendr\n",
      "nirm\n",
      "nschandr\n",
      "nu\n",
      "parsanjee\n",
      "pivnaa\n",
      "pt\n",
      "pyaswanak\n",
      "raa\n",
      "radh\n",
      "rafa\n",
      "raghura\n",
      "rajm\n",
      "rajuddi\n",
      "ramdhy\n",
      "rames\n",
      "ramr\n",
      "rpjuhlnaea\n",
      "rshp\n",
      "sahtanidh\n",
      "sakaa\n",
      "satn\n",
      "sattar\n",
      "shabash\n",
      "shadnit\n",
      "shayiara\n",
      "slafal\n",
      "sonpnu\n",
      "sp\n",
      "sudra\n",
      "sushris\n",
      "til\n",
      "tisa\n",
      "tnagi\n",
      "twnt\n",
      "uu\n",
      "vaeeknuzami\n",
      "vaka\n",
      "vaslee\n",
      "veree\n",
      "vidi\n",
      "vijk\n",
      "vikhariy\n",
      "vioo\n",
      "vish\n",
      "vs\n",
      "yashish\n",
      "yasrrit\n",
      "ynn\n",
      "zeb\n",
      "zitudi\n"
     ]
    }
   ],
   "source": [
    "generated_without_existing = sorted(list(set([x for i,x in enumerate(clean_output) if x not in list_existing])))\n",
    "print(f\"Quantity of new words generated: {len(generated_without_existing)} \\n\")\n",
    "for n in generated_without_existing:\n",
    "    print(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
